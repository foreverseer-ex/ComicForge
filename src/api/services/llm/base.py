"""
LLM æœåŠ¡åŸºç¡€æŠ½è±¡ç±»ã€‚

å®šä¹‰æ‰€æœ‰ LLM æœåŠ¡çš„ç»Ÿä¸€æ¥å£ã€‚
"""
import functools
import json
import uuid
from abc import ABC, abstractmethod
from typing import Optional, List, AsyncGenerator

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import ToolMessage
from langchain_core.tools import tool, BaseTool
from langgraph.graph.state import CompiledStateGraph
from loguru import logger
from pydantic_core import to_jsonable_python

from api.constants.llm import (
    DEVELOP_MODE_PROMPTS, MCP_TOOLS_GUIDE,
    ERROR_SD_FORGE_CONNECTION, ERROR_CONNECTION_FAILED, ERROR_TOOL_CALL_FAILED,
    ERROR_TIMEOUT_TEMPLATE, ERROR_CONNECTION_TEMPLATE,
    SESSION_INFO_TEMPLATE, TOOL_USAGE_REMINDER_TEMPLATE,
    SUMMARY_MESSAGE_TEMPLATE, ITERATION_GUIDE, ITERATION_PROMPT_TEMPLATE, FINAL_OPERATION_PROMPT_TEMPLATE,
)
from api.routers.actor import (
    create_actor, get_actor, list_actors, update_actor,
    remove_actor, get_tag_description, get_all_tag_descriptions,
    add_example, remove_example, generate_portrait, add_actor_portrait
)
from api.routers.draw import (
    get_loras, get_sd_models, get_options, set_options, generate, get_image,
)
from api.routers.llm import (
    add_choices, get_choices, clear_choices, start_iteration,
)
from api.routers.memory import (
    create_memory, get_memory, list_memories, update_memory,
    delete_memory, delete_all_memories, get_key_description, get_all_key_descriptions,
)
from api.routers.novel import (
    get_chapter_content, get_line_content, get_project_content,
)
# ============================================================================
# âš ï¸ å®‰å…¨è¦æ±‚ï¼šæ‰€æœ‰MCPå·¥å…·å‡½æ•°å¿…é¡»æ¥è‡ªroutersï¼Œä¸èƒ½ç›´æ¥è°ƒç”¨æœåŠ¡å‡½æ•°
# ============================================================================
# å¯¼å…¥æ‰€æœ‰è·¯ç”±å‡½æ•°ï¼ˆè¿™äº›å‡½æ•°ç»è¿‡è·¯ç”±å±‚éªŒè¯ï¼Œç¡®ä¿å®‰å…¨æ€§ï¼‰
# æ³¨æ„ï¼šç»å¯¹ä¸èƒ½å¯¼å…¥ services å±‚çš„å‡½æ•°ï¼Œåªèƒ½ä½¿ç”¨ routers å±‚çš„å‡½æ•°
from api.routers.project import (
    get_project, update_project, update_progress,
)
from api.routers.reader import (
    get_line, get_chapter_lines, get_lines_range, get_chapters,
    get_chapter, get_chapter_summary, put_chapter_summary, get_stats,
)
from api.schemas.chat import ChatMessage, ChatIteration
from api.services.db import MemoryService, HistoryService
from api.settings import app_settings


def tool_wrapper(func):
    """åŒ…è£…å·¥å…·å‡½æ•°ï¼Œç¡®ä¿è¿”å›å€¼ç¬¦åˆ OpenAI API è¦æ±‚ï¼Œå¹¶å¤„ç†å¼‚å¸¸ã€‚"""

    @functools.wraps(func)
    async def async_wrapper(*args, **kwargs):
        try:
            result = await func(*args, **kwargs)
            result = to_jsonable_python(result)
            if not isinstance(result, (list, dict)):
                # åŸºæœ¬ç±»å‹ï¼Œæ¯”å¦‚å­—ç¬¦ä¸²
                return [result]
            if len(result) == 0:
                return ['æ— ç»“æœ']
            return result
        except Exception as e:
            # æ•è·å·¥å…·è°ƒç”¨ä¸­çš„å¼‚å¸¸ï¼Œè¿”å›å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
            error_str = str(e).lower()
            error_type = type(e).__name__

            # æ£€æŸ¥æ˜¯å¦æ˜¯SD-Forgeè¿æ¥é”™è¯¯
            is_sd_forge_error = (
                    "sd-forge" in error_str or
                    "502" in error_str or
                    "bad gateway" in error_str or
                    "502 bad gateway" in error_str or
                    error_type == "HTTPException" and "502" in str(e)
            )

            if is_sd_forge_error:
                logger.warning(f"âš ï¸ SD-Forge è¿æ¥å¤±è´¥ï¼ˆå·¥å…·: {func.__name__}ï¼‰: {e}")
                return ERROR_SD_FORGE_CONNECTION

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯
            is_connection_error = (
                    "connection" in error_str or
                    "connect" in error_str or
                    "è¿æ¥" in error_str or
                    "network" in error_str or
                    "ç½‘ç»œ" in error_str or
                    error_type in ("APIConnectionError", "ConnectError", "ConnectTimeout", "ReadTimeout",
                                   "HTTPException")
            )

            if is_connection_error:
                logger.warning(f"âš ï¸ å·¥å…·è°ƒç”¨è¿æ¥å¤±è´¥ï¼ˆå·¥å…·: {func.__name__}ï¼‰: {e}")
                return ERROR_CONNECTION_FAILED.format(error=str(e))

            # å…¶ä»–é”™è¯¯
            logger.exception(f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥ï¼ˆå·¥å…·: {func.__name__}ï¼‰: {e}")
            return ERROR_TOOL_CALL_FAILED.format(error=str(e))

    return async_wrapper


class AbstractLlmService(ABC):
    """
    LLM æœåŠ¡æŠ½è±¡åŸºç±»ã€‚
    
    å®šä¹‰ç»Ÿä¸€çš„ LLM æ¥å£ï¼Œæ”¯æŒä¸åŒçš„æä¾›å•†ï¼ˆOpenAIã€Ollama ç­‰ï¼‰ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡ã€‚"""
        self.llm: Optional[BaseChatModel] = None
        self.agent: Optional[CompiledStateGraph] = None
        self.tools: List[BaseTool] = []
        self._initialize_tools()

        # å°è¯•åˆå§‹åŒ– LLM æœåŠ¡
        try:
            self.initialize_llm()
        except Exception as e:
            logger.exception(f"LLM æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼ˆå°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶é‡è¯•ï¼‰: {e}")

    def _initialize_tools(self):
        """åˆå§‹åŒ–å·¥å…·å‡½æ•°åˆ—è¡¨ã€‚"""

        all_functions = [
            # Project ç®¡ç†ï¼ˆåªå…è®¸æŸ¥è¯¢å’Œæ›´æ–°ï¼Œä¸å…è®¸åˆ›å»ºå’Œåˆ é™¤ï¼‰
            get_project, update_project, update_progress,
            # Actor ç®¡ç†
            create_actor, get_actor, list_actors, update_actor,
            remove_actor, add_example, remove_example, generate_portrait, add_actor_portrait,
            get_tag_description, get_all_tag_descriptions,
            # Memory ç®¡ç†
            create_memory, get_memory, list_memories, update_memory,
            delete_memory, delete_all_memories, get_key_description, get_all_key_descriptions,
            # Reader åŠŸèƒ½
            get_line, get_chapter_lines, get_lines_range, get_chapters,
            get_chapter, get_chapter_summary, put_chapter_summary, get_stats,
            # Novel å†…å®¹ç®¡ç†
            get_project_content, get_chapter_content, get_line_content,
            # Draw åŠŸèƒ½
            get_loras, get_sd_models, get_options, set_options, generate, get_image,
            # LLM è¾…åŠ©åŠŸèƒ½
            add_choices, get_choices, clear_choices,
            # è¿­ä»£æ¨¡å¼
            start_iteration,
        ]
        # å…ˆåŒ…è£…ï¼Œå†è½¬æ¢ä¸ºå·¥å…·
        self.tools = [tool(tool_wrapper(func)) for func in all_functions]
        logger.info(f"å·²åˆå§‹åŒ– {len(self.tools)} ä¸ªå·¥å…·å‡½æ•°")

    @abstractmethod
    def initialize_llm(self) -> bool:
        """
        åˆå§‹åŒ– LLM å®ä¾‹ã€‚
        
        ç”±å­ç±»å®ç°å…·ä½“çš„ LLM åˆå§‹åŒ–é€»è¾‘ï¼ˆå¦‚ ChatOpenAIã€ChatOllama ç­‰ï¼‰ã€‚
        
        :return: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        raise NotImplementedError

    def is_ready(self) -> bool:
        """
        æ£€æŸ¥æœåŠ¡æ˜¯å¦å°±ç»ªã€‚
        
        :return: æœåŠ¡æ˜¯å¦å·²åˆå§‹åŒ–ä¸”å¯ç”¨
        """
        return self.llm is not None and self.agent is not None

    def get_session_context(self, project_id: str) -> Optional[str]:
        """
        è·å–å½“å‰é¡¹ç›®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…æ‹¬é¡¹ç›®åŸºæœ¬ä¿¡æ¯å’Œæ‰€æœ‰è®°å¿†æ¡ç›®ï¼‰ã€‚
        
        :param project_id: é¡¹ç›® ID
        :return: æ ¼å¼åŒ–çš„é¡¹ç›®ä¿¡æ¯ï¼ˆJSON å­—ç¬¦ä¸²ï¼‰ï¼Œå¦‚æœé¡¹ç›®ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        try:
            # æŸ¥è¯¢é¡¹ç›®ä¿¡æ¯ï¼ˆé¡¹ç›®ä¸å­˜åœ¨æ—¶è¿”å› Noneï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼‰
            from api.services.db.project_service import ProjectService
            project = ProjectService.get(project_id)
            if not project:
                logger.debug(f"é¡¹ç›®ä¸å­˜åœ¨: {project_id}ï¼Œè·³è¿‡é¡¹ç›®ä¸Šä¸‹æ–‡")
                return None

            # æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = {
                "project_id": project.project_id,
                "title": project.title,
                "novel_path": project.novel_path,
                "total_lines": project.total_lines,
                "total_chapters": project.total_chapters,
                "current_line": project.current_line,
                "current_chapter": project.current_chapter,
            }

            # æŸ¥è¯¢æ‰€æœ‰è®°å¿†æ¡ç›®
            memories = MemoryService.list(project_id=project_id, limit=1000)

            # æ„å»ºè®°å¿†å­—å…¸ï¼ˆæŒ‰ key åˆ†ç»„ï¼‰
            memories_dict = {}
            for memory in memories:
                memories_dict[memory.key] = {
                    "value": memory.value,
                    "description": memory.description,
                }

            # æ ¼å¼åŒ–ä¸º JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«æç¤ºä¿¡æ¯
            session_info = SESSION_INFO_TEMPLATE.format(
                context_json=json.dumps(context, ensure_ascii=False, indent=2),
                memories_count=len(memories),
                memories_dict_json=json.dumps(memories_dict, ensure_ascii=False, indent=2),
            )

            return session_info

        except Exception as e:
            logger.exception(f"è·å–é¡¹ç›®ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return None

    def build_system_messages(self, project_id: str) -> list[tuple[str, str]]:
        """
        æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«é¡¹ç›®ä¸Šä¸‹æ–‡å’Œç”¨æˆ·æ¶ˆæ¯ã€‚

        :param project_id: é¡¹ç›® ID
        :return: åŒ…å«ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æ¶ˆæ¯çš„å…ƒç»„åˆ—è¡¨
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []

        # 1. å¦‚æœå¯ç”¨å¼€å‘è€…æ¨¡å¼ï¼Œæ·»åŠ å¼€å‘è€…æ¨¡å¼æç¤ºè¯
        if app_settings.llm.developer_mode:
            messages.append(("system", DEVELOP_MODE_PROMPTS))

        # 2. å¦‚æœé…ç½®äº†ç³»ç»Ÿæç¤ºè¯ï¼ˆéç©ºï¼‰ï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if app_settings.llm.system_prompt and app_settings.llm.system_prompt.strip():
            messages.append(("system", app_settings.llm.system_prompt))

        # 3. æ·»åŠ å¼ºåˆ¶å·¥å…·è°ƒç”¨æç¤º
        tool_usage_reminder = TOOL_USAGE_REMINDER_TEMPLATE.format(tools_count=len(self.tools))
        messages.append(("system", tool_usage_reminder))

        # 4. æ·»åŠ  MCP å·¥å…·ä½¿ç”¨æŒ‡å—
        messages.append(("system", MCP_TOOLS_GUIDE))

        # 5. æ·»åŠ å½“å‰é¡¹ç›®ä¿¡æ¯
        session_info = self.get_session_context(project_id)
        if session_info:
            messages.append(("system", session_info))

        # åˆå¹¶è®°å½•æ‰€æœ‰ç³»ç»Ÿæç¤ºè¯
        if len(messages) > 0:
            logger.debug(
                f"å·²æ·»åŠ  {len(messages)} æ¡ç³»ç»Ÿæç¤ºè¯ï¼ˆå¼€å‘è€…æ¨¡å¼ã€ç³»ç»Ÿæç¤ºè¯ã€å·¥å…·è°ƒç”¨æç¤ºã€MCPæŒ‡å—ã€é¡¹ç›®ä¿¡æ¯ã€å†å²è­¦å‘Šï¼‰")
        return messages

    def summary_history(self, project_id: str):
        """
        è‡ªåŠ¨ç”ŸæˆèŠå¤©æ‘˜è¦ï¼ˆå½“æ¶ˆæ¯æ•°é‡è¾¾åˆ° summary_epoch çš„å€æ•°æ—¶ï¼‰ã€‚
        
        :param project_id: é¡¹ç›®ID
        """
        if not self.is_ready():
            return

        count = HistoryService.count(project_id)
        res_round = count % app_settings.llm.summary_epoch

        if count > 0 and res_round == 0:
            try:
                # è·å–æœ€è¿‘çš„æ¶ˆæ¯
                start_index = count - app_settings.llm.summary_epoch
                messages = self.build_system_messages(project_id)
                recent_messages = HistoryService.list(project_id, start_index=start_index, end_index=count)

                # è·å–ç°æœ‰æ‘˜è¦
                chat_summary = MemoryService.get_summary(project_id)
                summary_value = chat_summary.data if chat_summary and chat_summary.data else ""

                # æ„å»ºæ‘˜è¦æç¤ºè¯
                summary_message = SUMMARY_MESSAGE_TEMPLATE.format(
                    previous_rounds=count - app_settings.llm.summary_epoch,
                    summary_value=summary_value,
                    recent_rounds=app_settings.llm.summary_epoch,
                    recent_messages=json.dumps(
                        [{"role": msg.role, "context": msg.context} for msg in recent_messages],
                        ensure_ascii=False,
                        indent=2
                    ),
                )
                messages.append(("system", summary_message))

                # è°ƒç”¨ agent ç”Ÿæˆæ‘˜è¦
                result = self.agent.invoke({"messages": messages})

                # æå–æ‘˜è¦æ–‡æœ¬
                summary_text = ""
                if hasattr(result, "messages") and result.messages:
                    last_msg = result.messages[-1]
                    if hasattr(last_msg, "content"):
                        summary_text = last_msg.content
                    elif isinstance(last_msg, dict) and "content" in last_msg:
                        summary_text = last_msg["content"]

                # ä¿å­˜æ‘˜è¦
                if summary_text:
                    MemoryService.update_summary(project_id, summary_text)
                    logger.info(f"å·²ç”ŸæˆèŠå¤©æ‘˜è¦: {project_id}, é•¿åº¦={len(summary_text)}")
            except Exception as e:
                logger.exception(f"ç”ŸæˆèŠå¤©æ‘˜è¦å¤±è´¥: {e}")

    async def chat_streamed(self, message: str, project_id: str) -> AsyncGenerator[dict, None]:
        """
        å¢å¼ºçš„æµå¼å¯¹è¯æ–¹æ³•ï¼Œè¿”å›ç»“æ„åŒ–äº‹ä»¶ã€‚
        
        :param message: ç”¨æˆ·æ¶ˆæ¯
        :param project_id: é¡¹ç›® ID
        :yield: äº‹ä»¶å­—å…¸ï¼ŒåŒ…å« type å’Œç›¸åº”çš„æ•°æ®
        """
        logger.info(f"ğŸ‘¤ ç”¨æˆ·æ¶ˆæ¯: {message[:200]}{'...' if len(message) > 200 else ''}")

        if not self.is_ready():
            logger.error("LLM æœåŠ¡æœªå°±ç»ª")
            error_msg = ChatMessage(
                message_id=str(uuid.uuid4()),
                project_id=project_id,
                role="assistant",
                context="é”™è¯¯ï¼šLLM æœåŠ¡æœªå°±ç»ªï¼Œè¯·å…ˆåˆå§‹åŒ– LLM",
                status="error",
                message_type="normal"
            )
            HistoryService.create(error_msg)
            yield {'type': 'error', 'error': 'LLM æœåŠ¡æœªå°±ç»ªï¼Œè¯·å…ˆåˆå§‹åŒ– LLM'}
            return

        # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯çš„ IDï¼ˆæå‰åˆ›å»ºï¼Œç”¨äºå®æ—¶æ›´æ–°ï¼‰
        assistant_message_id = str(uuid.uuid4())
        assistant_context = ""
        assistant_tools: list[dict] = []
        assistant_suggests: list[str] = []

        try:
            # 1. æ„å»ºç³»ç»Ÿæ¶ˆæ¯å’Œå†å²æ¶ˆæ¯
            messages = self.build_system_messages(project_id)
            self.summary_history(project_id)
            chat_summary_memory = MemoryService.get_summary(project_id)

            # 2. åˆ›å»ºç”¨æˆ·æ¶ˆæ¯å¹¶å†™å…¥æ•°æ®åº“
            user_message = ChatMessage(
                message_id=str(uuid.uuid4()),
                project_id=project_id,
                role="user",
                context=message,
                status="ready",
                message_type="normal"
            )
            HistoryService.create(user_message)

            # 3. è·å–å†å²æ¶ˆæ¯ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
            start_index = max(0, HistoryService.count(project_id) - app_settings.llm.summary_epoch)
            messages_to_include = HistoryService.list(project_id, start_index=start_index)

            # 4. å¦‚æœæœ‰èŠå¤©æ‘˜è¦ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯
            if chat_summary_memory and chat_summary_memory.data:
                summary_message = SUMMARY_MESSAGE_TEMPLATE.format(
                    previous_rounds=start_index,
                    summary_value=chat_summary_memory.data,
                    recent_rounds=len(messages_to_include),
                )
                messages.append(("system", summary_message))

            # 5. æ·»åŠ å†å²æ¶ˆæ¯åˆ° langchain æ¶ˆæ¯åˆ—è¡¨
            for msg in messages_to_include:
                if msg.role == "user":
                    messages.append(("human", msg.context))
                elif msg.role == "assistant":
                    messages.append(("ai", msg.context))

            # 6. åˆ›å»ºåˆå§‹åŠ©æ‰‹æ¶ˆæ¯ï¼ˆçŠ¶æ€ä¸º thinkingï¼‰
            assistant_message = ChatMessage(
                message_id=assistant_message_id,
                project_id=project_id,
                role="assistant",
                context="",
                status="thinking",
                message_type="normal",
                tools=[],
                suggests=[]
            )
            HistoryService.create(assistant_message)

            # å‘é€æ¶ˆæ¯ID
            yield {'type': 'message_id', 'message_id': assistant_message_id}
            yield {'type': 'status', 'status': 'thinking'}

            # 7. å®ç°æµå¼å¯¹è¯é€»è¾‘
            logger.info(f"å¼€å§‹æµå¼å¯¹è¯ï¼Œä½¿ç”¨ {len(self.tools)} ä¸ªå·¥å…·")
            config = {"recursion_limit": app_settings.llm.recursion_limit}

            async for chunk in self.agent.astream_events(
                    {"messages": messages},
                    version="v2",
                    config=config
            ):
                event_type = chunk.get("event")

                # å¤„ç†æ–‡æœ¬æµäº‹ä»¶
                if event_type == "on_chat_model_stream":
                    message_chunk = chunk.get("data", {}).get("chunk")
                    if message_chunk and hasattr(message_chunk, "content") and message_chunk.content:
                        content = message_chunk.content
                        assistant_context += content

                        # å®æ—¶æ›´æ–°æ•°æ®åº“ä¸­çš„åŠ©æ‰‹æ¶ˆæ¯
                        assistant_message.context = assistant_context
                        assistant_message.tools = assistant_tools.copy()
                        HistoryService.update(assistant_message)

                        yield {'type': 'content', 'content': content}

                # å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶
                elif event_type == "on_tool_start":
                    self._process_tool_start_event(chunk, assistant_tools)

                    # æ›´æ–°æ•°æ®åº“
                    assistant_message.tools = assistant_tools.copy()
                    HistoryService.update(assistant_message)

                    # å‘é€å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶
                    tool_name = chunk.get("name", "")
                    tool_input = chunk.get("data", {}).get("input", {})
                    yield {
                        'type': 'tool_start',
                        'name': tool_name,
                        'args': tool_input if isinstance(tool_input, dict) else {}
                    }
                    # å‘é€å®Œæ•´çš„å·¥å…·åˆ—è¡¨
                    yield {'type': 'tools', 'tools': assistant_tools.copy()}

                # å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸäº‹ä»¶
                elif event_type == "on_tool_end":
                    tool_output: ToolMessage = chunk.get("data", {}).get("output")
                    logger.info(
                        f"âœ… å·¥å…·è°ƒç”¨å®Œæˆ: {chunk.get('name', '')}, ç»“æœé•¿åº¦={len(str(tool_output.content)) if tool_output.content else 0}")

                    self._process_tool_end_event(chunk, assistant_tools)

                    # æ›´æ–°æ•°æ®åº“
                    assistant_message.tools = assistant_tools.copy()
                    HistoryService.update(assistant_message)

                    # å‘é€å·¥å…·è°ƒç”¨ç»“æŸäº‹ä»¶
                    tool_name = chunk.get("name", "")
                    yield {
                        'type': 'tool_end',
                        'name': tool_name,
                        'result': tool_output.content
                    }
                    # å‘é€å®Œæ•´çš„å·¥å…·åˆ—è¡¨
                    yield {'type': 'tools', 'tools': assistant_tools.copy()}

                    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœå·¥å…·æ˜¯ add_choicesï¼Œæ›´æ–° suggests
                    if tool_name == "add_choices":
                        from api.routers.llm import _session_choices
                        choices = _session_choices.get(project_id, [])
                        suggests = []
                        for choice in choices:
                            if isinstance(choice, dict):
                                if choice.get("type") == "image":
                                    suggests.append(f"image:{choice.get('url', '')}")
                                elif choice.get("type") == "text":
                                    suggests.append(choice.get("text", ""))
                            elif isinstance(choice, str):
                                suggests.append(choice)
                        assistant_message.suggests = suggests
                        assistant_suggests = suggests
                        HistoryService.update(assistant_message)

                        # å‘é€å»ºè®®æ›´æ–°
                        yield {'type': 'suggests', 'suggests': suggests}

            # 8. å¯¹è¯å®Œæˆï¼Œæ›´æ–°åŠ©æ‰‹æ¶ˆæ¯çŠ¶æ€ä¸º ready
            assistant_message.status = "ready"
            assistant_message.context = assistant_context
            assistant_message.tools = assistant_tools.copy()
            HistoryService.update(assistant_message)

            yield {'type': 'status', 'status': 'ready'}
            logger.info(f"âœ… å¯¹è¯å®Œæˆ: {len(assistant_context)} å­—ç¬¦, {len(assistant_tools)} ä¸ªå·¥å…·è°ƒç”¨")

        except Exception as e:
            logger.exception(f"å¯¹è¯å¤±è´¥: {e}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯æˆ–è¶…æ—¶é”™è¯¯
            error_type = type(e).__name__
            error_str = str(e).lower()

            is_connection_error = (
                    "connection" in error_str or
                    "connect" in error_str or
                    "è¿æ¥" in error_str or
                    "network" in error_str or
                    "ç½‘ç»œ" in error_str or
                    error_type in ("APIConnectionError", "ConnectError", "ConnectTimeout", "ReadTimeout")
            )

            is_timeout_error = (
                    "timeout" in error_str or
                    "è¶…æ—¶" in error_str or
                    error_type in ("TimeoutError", "ConnectTimeout", "ReadTimeout")
            )

            if is_connection_error or is_timeout_error:
                if is_timeout_error:
                    error_msg = ERROR_TIMEOUT_TEMPLATE.format(timeout=app_settings.llm.timeout)
                else:
                    error_msg = ERROR_CONNECTION_TEMPLATE
            else:
                error_msg = f"é”™è¯¯ï¼š{e}"

            # æ›´æ–°åŠ©æ‰‹æ¶ˆæ¯ä¸ºé”™è¯¯çŠ¶æ€
            assistant_message.status = "error"
            assistant_message.context = error_msg
            HistoryService.update(assistant_message)

            yield {'type': 'status', 'status': 'error'}
            yield {'type': 'error', 'error': error_msg}

    async def chat_text_only(self, message: str, project_id: str) -> AsyncGenerator[str, None]:
        """
        ä¸ LLM è¿›è¡Œæ ‡å‡†å¯¹è¯ï¼ˆæµå¼è¿”å›ï¼‰ã€‚
        
        è¿™æ˜¯ `chat_streamed` çš„åŒ…è£…å™¨ï¼Œåªè¿”å›æ–‡æœ¬å†…å®¹ç‰‡æ®µï¼Œä¿æŒå‘åå…¼å®¹ã€‚
        å†…éƒ¨è°ƒç”¨ `chat_streamed` å¹¶æå– `content` äº‹ä»¶ã€‚
        
        :param message: ç”¨æˆ·æ¶ˆæ¯
        :param project_id: é¡¹ç›® ID
        :yield: LLM å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        async for event in self.chat_streamed(message, project_id):
            if event.get('type') == 'content':
                yield event.get('content', '')
            elif event.get('type') == 'error':
                yield event.get('error', 'é”™è¯¯ï¼šæœªçŸ¥é”™è¯¯')
                return

    async def chat_iteration(self, iteration_data: dict, project_id: str) -> AsyncGenerator[str, None]:
        """
        è¿­ä»£æ¨¡å¼ä¸“ç”¨æ–¹æ³•ã€‚
        
        å®Œå…¨åŸºäºæ•°æ®åº“æ“ä½œï¼š
        - è¿­ä»£æ•°æ®å­˜å‚¨åœ¨ ChatMessage çš„ data å­—æ®µä¸­ï¼ˆChatIteration å¯¹è±¡ï¼‰
        - å®æ—¶æ›´æ–°è¿­ä»£è¿›åº¦åˆ°æ•°æ®åº“
        - æœ€ç»ˆæ“ä½œçš„å·¥å…·è°ƒç”¨è®°å½•åˆ° tools å­—æ®µ
        
        é€šç”¨è¿­ä»£ç®¡ç†ï¼š
        1. æ¯æ¬¡è¿­ä»£ï¼Œè®© LLM è‡ªè¡Œè°ƒç”¨å·¥å…·å¤„ç†å½“å‰ index çš„å†…å®¹
        2. LLM å¤„ç†å®Œæˆåï¼Œæ›´æ–° index += step
        3. ç´¯ç§¯ summary
        4. å½“ index >= stop æ—¶ï¼Œæ‰§è¡Œæœ€ç»ˆæ“ä½œå¹¶é€€å‡º
        
        :param iteration_data: è¿­ä»£æ•°æ®å­—å…¸ï¼ˆåŒ…å« message_id æˆ–å®Œæ•´çš„è¿­ä»£ä¿¡æ¯ï¼‰
        :param project_id: é¡¹ç›®ID
        :yield: LLMå“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        # 1. è·å–æˆ–åˆ›å»ºè¿­ä»£æ¶ˆæ¯
        if "message_id" in iteration_data:
            # ä»æ•°æ®åº“è¯»å–ç°æœ‰è¿­ä»£æ¶ˆæ¯
            iteration_message = HistoryService.get(iteration_data["message_id"])
            if not iteration_message or iteration_message.project_id != project_id:
                logger.error(f"è¿­ä»£æ¶ˆæ¯ä¸å­˜åœ¨: {iteration_data['message_id']}")
                yield "é”™è¯¯ï¼šè¿­ä»£æ¶ˆæ¯ä¸å­˜åœ¨"
                return
            iteration = ChatIteration(**iteration_message.data)
        else:
            # åˆ›å»ºæ–°çš„è¿­ä»£æ¶ˆæ¯
            iteration = ChatIteration(**iteration_data)
            iteration_message = ChatMessage(
                message_id=str(uuid.uuid4()),
                project_id=project_id,
                role="assistant",
                context="",
                status="thinking",
                message_type="iteration",
                data=iteration.model_dump(),
                tools=[],
                suggests=[]
            )
            HistoryService.create(iteration_message)

        # 2. è¿­ä»£å¾ªç¯
        while iteration.index < iteration.stop:
            # æ„å»ºè¿­ä»£æ¨¡å¼ä¸“ç”¨æç¤ºè¯
            iteration_prompt = self._build_iteration_prompt(iteration, project_id)

            # è°ƒç”¨LLMå¤„ç†å½“å‰è¿­ä»£ï¼ˆè¿­ä»£è¿‡ç¨‹ä¸­ä¸è®°å½•å·¥å…·è°ƒç”¨ï¼Œåªç´¯ç§¯summaryï¼‰
            full_response = ""
            async for chunk in self._call_llm_in_iteration_mode(iteration_prompt, project_id):
                full_response += chunk
                yield chunk

            # æ›´æ–°summaryï¼ˆå°†LLMçš„å“åº”è¿½åŠ åˆ°summaryï¼‰
            if iteration.summary:
                iteration.summary += "\n\n"
            iteration.summary += f"[ç¬¬ {iteration.index // iteration.step + 1} æ¬¡è¿­ä»£] {full_response}"

            # æ›´æ–°indexï¼ˆå°†æ­¥é•¿å åŠ åˆ°indexï¼‰
            iteration.index += iteration.step

            # æ›´æ–°æ•°æ®åº“ä¸­çš„è¿­ä»£æ¶ˆæ¯
            iteration_message.data = iteration.model_dump()
            iteration_message.status = "thinking"
            HistoryService.update(iteration_message)

            logger.debug(
                f"å·²æ›´æ–°è¿­ä»£æ¶ˆæ¯è¿›åº¦: index={iteration.index}/{iteration.stop}, "
                f"summaryé•¿åº¦={len(iteration.summary) if iteration.summary else 0}")

            # æ£€æŸ¥æ˜¯å¦è¿­ä»£ç»ˆæ­¢
            if iteration.index >= iteration.stop:
                break

        # 3. è¿­ä»£å®Œæˆï¼Œæ‰§è¡Œæœ€ç»ˆæ“ä½œ
        logger.info(f"è¿­ä»£å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œæœ€ç»ˆæ“ä½œï¼š{iteration.target}")
        final_prompt = self._build_final_operation_prompt(iteration)

        # æ¸…ç©ºä¹‹å‰çš„å·¥å…·è°ƒç”¨ï¼ˆåªä¿ç•™æœ€ç»ˆæ“ä½œçš„å·¥å…·è°ƒç”¨ï¼‰
        final_tools: list[dict] = []
        final_context_ref = [""]  # ä½¿ç”¨åˆ—è¡¨å¼•ç”¨ä»¥ä¾¿ä¿®æ”¹

        # 4. è°ƒç”¨LLMæ‰§è¡Œæœ€ç»ˆæ“ä½œï¼ˆè®°å½•å·¥å…·è°ƒç”¨ï¼‰
        async for chunk in self._call_llm_final_operation(final_prompt, project_id, final_tools, final_context_ref):
            yield chunk

        # 5. æ›´æ–°è¿­ä»£æ¶ˆæ¯ï¼ˆæ ‡è®°ä¸ºå®Œæˆï¼ŒåŒ…å«æœ€ç»ˆæ“ä½œçš„å·¥å…·è°ƒç”¨ï¼‰
        iteration_message.status = "ready"
        iteration_message.context = final_context_ref[0]
        iteration_message.tools = final_tools.copy()
        iteration_message.data = iteration.model_dump()
        HistoryService.update(iteration_message)

        logger.info(
            f"âœ… è¿­ä»£å®Œæˆ: index={iteration.index}/{iteration.stop}, "
            f"summaryé•¿åº¦={len(iteration.summary)}, å·¥å…·è°ƒç”¨æ•°={len(final_tools)}")

    def _build_iteration_prompt(self, iteration: ChatIteration, project_id: str) -> str:
        """æ„å»ºè¿­ä»£æ¨¡å¼æç¤ºè¯"""
        progress_percent = iteration.index * 100 // iteration.stop if iteration.stop > 0 else 0
        is_near_completion = "æ˜¯" if iteration.index + iteration.step >= iteration.stop else "å¦"
        summary_display = iteration.summary if iteration.summary else "ï¼ˆæš‚æ— ï¼‰"

        return ITERATION_PROMPT_TEMPLATE.format(
            target=iteration.target,
            index=iteration.index,
            step=iteration.step,
            stop=iteration.stop,
            progress_percent=progress_percent,
            is_near_completion=is_near_completion,
            summary_display=summary_display,
        )

    def _build_final_operation_prompt(self, iteration: ChatIteration) -> str:
        """æ„å»ºæœ€ç»ˆæ“ä½œæç¤ºè¯"""
        iterations_count = iteration.stop // iteration.step if iteration.step > 0 else 0

        return FINAL_OPERATION_PROMPT_TEMPLATE.format(
            target=iteration.target,
            stop=iteration.stop,
            iterations_count=iterations_count,
            summary=iteration.summary,
        )

    async def _call_llm_in_iteration_mode(self, prompt: str, project_id: str) -> AsyncGenerator[str, None]:
        """
        åœ¨è¿­ä»£æ¨¡å¼ä¸‹è°ƒç”¨LLMï¼ˆä¸è®°å½•åˆ°historyï¼Œåªç”¨äºç´¯ç§¯summaryï¼‰ã€‚
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if app_settings.llm.developer_mode:
            messages.append(("system", DEVELOP_MODE_PROMPTS))

        if app_settings.llm.system_prompt:
            messages.append(("system", app_settings.llm.system_prompt))

        # æ·»åŠ è¿­ä»£æ¨¡å¼ä¸“ç”¨æŒ‡å—
        messages.append(("system", ITERATION_GUIDE))

        # æ·»åŠ å½“å‰æç¤ºè¯
        messages.append(("human", prompt))

        # é…ç½®é€’å½’é™åˆ¶ï¼ˆç”¨äºè¿­ä»£æ¨¡å¼ç­‰éœ€è¦å¤§é‡å·¥å…·è°ƒç”¨çš„åœºæ™¯ï¼‰
        config = {"recursion_limit": app_settings.llm.recursion_limit}

        # è°ƒç”¨LLMï¼ˆä¸è®°å½•åˆ°historyï¼‰
        full_response = ""
        async for chunk in self.agent.astream_events({"messages": messages}, version="v2", config=config):
            event_type = chunk.get("event")

            if event_type == "on_chat_model_stream":
                message_chunk = chunk.get("data", {}).get("chunk")
                if message_chunk and hasattr(message_chunk, "content"):
                    content = message_chunk.content
                    if content:
                        full_response += content
                        yield content

        # è¿­ä»£è¿‡ç¨‹ä¸­çš„å·¥å…·è°ƒç”¨ä¸éœ€è¦è®°å½•ï¼ˆç”¨æˆ·è¦æ±‚ï¼‰

    def _process_tool_start_event(self, chunk: dict, tools_list: list, log_prefix: str = "") -> None:
        """
        å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶çš„é€šç”¨æ–¹æ³•ã€‚
        
        :param chunk: äº‹ä»¶ chunk
        :param tools_list: å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆä¼šè¢«æ›´æ–°ï¼‰
        :param log_prefix: æ—¥å¿—å‰ç¼€ï¼ˆå¯é€‰ï¼‰
        """
        tool_name = chunk.get("name", "")
        tool_input = chunk.get("data", {}).get("input", {})
        prefix = f"[{log_prefix}] " if log_prefix else ""
        logger.info(f"âœ… {prefix}å·¥å…·è°ƒç”¨: {tool_name}, å‚æ•°: {tool_input}")

        args = tool_input if isinstance(tool_input, dict) else {}
        if 'request' in args and len(args) == 1:
            args = args['request']

        tool_call = {
            "name": tool_name,
            "args": args,
            "result": None
        }
        tools_list.append(tool_call)

    def _process_tool_end_event(self, chunk: dict, tools_list: list, log_prefix: str = "") -> None:
        """
        å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸäº‹ä»¶çš„é€šç”¨æ–¹æ³•ã€‚
        
        :param chunk: äº‹ä»¶ chunk
        :param tools_list: å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆä¼šè¢«æ›´æ–°ï¼‰
        :param log_prefix: æ—¥å¿—å‰ç¼€ï¼ˆå¯é€‰ï¼‰
        """
        tool_name = chunk.get("name", "")
        tool_output: ToolMessage = chunk.get("data", {}).get("output")
        prefix = f"[{log_prefix}] " if log_prefix else ""
        logger.info(f"âœ… {prefix}å·¥å…·è°ƒç”¨å®Œæˆ: {tool_name}")

        if tools_list:
            try:
                result = json.loads(tool_output.content)
            except (json.JSONDecodeError, TypeError):
                # å¯èƒ½æ˜¯åŸºæœ¬ç±»å‹ï¼Œæ¯”å¦‚å­—ç¬¦ä¸²
                result = tool_output.content
            tools_list[-1]["result"] = result
            tools_list[-1]["status"] = tool_output.status
            tools_list[-1]["tool_call_id"] = tool_output.tool_call_id

    async def _call_llm_final_operation(
            self,
            prompt: str,
            project_id: str,
            tools_list: list,
            context_ref: list
    ) -> AsyncGenerator[str, None]:
        """
        è°ƒç”¨LLMæ‰§è¡Œæœ€ç»ˆæ“ä½œï¼ˆå…è®¸æ‰€æœ‰å·¥å…·ï¼Œè®°å½•åˆ°ä¼ å…¥çš„åˆ—è¡¨ï¼‰ã€‚
        
        è¿™æ˜¯ `chat_streamed` çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ¶ˆæ¯åˆ—è¡¨ï¼Œä¸å†™å…¥æ•°æ®åº“ã€‚
        
        :param prompt: æç¤ºè¯
        :param project_id: é¡¹ç›®ID
        :param tools_list: å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆä¼šè¢«å®æ—¶æ›´æ–°ï¼‰
        :param context_ref: ä¸Šä¸‹æ–‡å†…å®¹çš„å¼•ç”¨ï¼ˆåˆ—è¡¨ï¼Œç”¨äºä¿®æ”¹å­—ç¬¦ä¸²ï¼‰
        """
        # æ„å»ºè‡ªå®šä¹‰æ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸åŒ…å«å†å²æ¶ˆæ¯å’ŒèŠå¤©æ‘˜è¦ï¼‰
        messages = []

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if app_settings.llm.developer_mode:
            messages.append(("system", DEVELOP_MODE_PROMPTS))

        if app_settings.llm.system_prompt:
            messages.append(("system", app_settings.llm.system_prompt))

        # æ·»åŠ  MCP å·¥å…·ä½¿ç”¨æŒ‡å—
        messages.append(("system", MCP_TOOLS_GUIDE))

        # æ·»åŠ å½“å‰é¡¹ç›®ä¿¡æ¯
        session_info = self.get_session_context(project_id)
        if session_info:
            messages.append(("system", session_info))

        # æ·»åŠ å½“å‰æç¤ºè¯
        messages.append(("human", prompt))

        # é…ç½®é€’å½’é™åˆ¶
        config = {"recursion_limit": app_settings.llm.recursion_limit}

        # è°ƒç”¨LLMå¹¶å¤„ç†äº‹ä»¶
        async for chunk in self.agent.astream_events({"messages": messages}, version="v2", config=config):
            event_type = chunk.get("event")

            if event_type == "on_chat_model_stream":
                message_chunk = chunk.get("data", {}).get("chunk")
                if message_chunk and hasattr(message_chunk, "content") and message_chunk.content:
                    content = message_chunk.content
                    context_ref[0] += content
                    yield content

            elif event_type == "on_tool_start":
                self._process_tool_start_event(chunk, tools_list, "æœ€ç»ˆæ“ä½œ")

            elif event_type == "on_tool_end":
                self._process_tool_end_event(chunk, tools_list, "æœ€ç»ˆæ“ä½œ")
