"""
LLM æœåŠ¡åŸºç¡€æŠ½è±¡ç±»ã€‚

å®šä¹‰æ‰€æœ‰ LLM æœåŠ¡çš„ç»Ÿä¸€æ¥å£ã€‚
"""
import functools
import json
from abc import ABC, abstractmethod
from typing import Optional, List, AsyncGenerator, Any

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import ToolMessage, AIMessage
from langchain_core.tools import tool, BaseTool
from langgraph.graph.state import CompiledStateGraph
from loguru import logger
from pydantic_core import to_jsonable_python
from api.services.db.project_service import ProjectService
from api.constants.llm import (
    DEVELOP_MODE_PROMPTS, MCP_TOOLS_GUIDE,
    ERROR_SD_FORGE_CONNECTION, ERROR_CONNECTION_FAILED, ERROR_TOOL_CALL_FAILED,
    ERROR_TIMEOUT_TEMPLATE, ERROR_CONNECTION_TEMPLATE,
    SESSION_INFO_TEMPLATE,
    SUMMARY_MESSAGE_TEMPLATE, ITERATION_GUIDE, ITERATION_PROMPT_TEMPLATE, FINAL_OPERATION_PROMPT_TEMPLATE,
)
from api.routers.actor import (
    create_actor, get_actor, get_all_actors, update_actor,
    remove_actor, get_tag_description, get_all_tag_descriptions,
    add_example, remove_example, add_portrait_from_batch_tool, add_portrait_from_job_tool
)
from api.routers.draw import (
    create_draw_job, create_batch_job, batch_from_jobs,
    get_draw_job, delete_draw_job, get_image,
)
from api.routers.model_meta import (
    get_loras, get_checkpoints,
)

from api.routers.memory import (
    create_memory, get_memory, get_all_memories, update_memory,
    delete_memory, clear_memories, get_key_description, get_all_key_descriptions,
)
# ============================================================================
# âš ï¸ å®‰å…¨è¦æ±‚ï¼šæ‰€æœ‰MCPå·¥å…·å‡½æ•°å¿…é¡»æ¥è‡ªroutersï¼Œä¸èƒ½ç›´æ¥è°ƒç”¨æœåŠ¡å‡½æ•°
# ============================================================================
# å¯¼å…¥æ‰€æœ‰è·¯ç”±å‡½æ•°ï¼ˆè¿™äº›å‡½æ•°ç»è¿‡è·¯ç”±å±‚éªŒè¯ï¼Œç¡®ä¿å®‰å…¨æ€§ï¼‰
# æ³¨æ„ï¼šç»å¯¹ä¸èƒ½å¯¼å…¥ services å±‚çš„å‡½æ•°ï¼Œåªèƒ½ä½¿ç”¨ routers å±‚çš„å‡½æ•°
from api.routers.project import (
    get_project, update_project,
)
from api.routers.context import (
    get_line, get_chapter_lines, get_lines_range, get_chapters,
    get_chapter, update_chapter, get_stats, get_project_content,
)
from api.schemas.chat import ChatMessage, ChatIteration
from api.services.db import MemoryService, HistoryService
from api.settings import app_settings


def tool_wrapper(func):
    """åŒ…è£…å·¥å…·å‡½æ•°ï¼Œç¡®ä¿è¿”å›å€¼ç¬¦åˆ OpenAI API è¦æ±‚ï¼Œå¹¶å¤„ç†å¼‚å¸¸ã€‚"""

    @functools.wraps(func)
    async def async_wrapper(*args, **kwargs):
        try:
            result = await func(*args, **kwargs)
            result = to_jsonable_python(result)
            if not isinstance(result, (list, dict)):
                # åŸºæœ¬ç±»å‹ï¼Œæ¯”å¦‚å­—ç¬¦ä¸²
                return [result]
            if len(result) == 0:
                return ['æ— ç»“æœ']
            return result
        except Exception as e:
            # æ•è·å·¥å…·è°ƒç”¨ä¸­çš„å¼‚å¸¸ï¼Œè¿”å›å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
            error_str = str(e).lower()
            error_type = type(e).__name__

            # æ£€æŸ¥æ˜¯å¦æ˜¯SD-Forgeè¿æ¥é”™è¯¯
            is_sd_forge_error = (
                    "sd-forge" in error_str or
                    "502" in error_str or
                    "bad gateway" in error_str or
                    "502 bad gateway" in error_str or
                    error_type == "HTTPException" and "502" in str(e)
            )

            if is_sd_forge_error:
                logger.warning(f"âš ï¸ SD-Forge è¿æ¥å¤±è´¥ï¼ˆå·¥å…·: {func.__name__}ï¼‰: {e}")
                return ERROR_SD_FORGE_CONNECTION

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯
            is_connection_error = (
                    "connection" in error_str or
                    "connect" in error_str or
                    "è¿æ¥" in error_str or
                    "network" in error_str or
                    "ç½‘ç»œ" in error_str or
                    error_type in ("APIConnectionError", "ConnectError", "ConnectTimeout", "ReadTimeout",
                                   "HTTPException")
            )

            if is_connection_error:
                logger.warning(f"âš ï¸ å·¥å…·è°ƒç”¨è¿æ¥å¤±è´¥ï¼ˆå·¥å…·: {func.__name__}ï¼‰: {e}")
                return ERROR_CONNECTION_FAILED.format(error=str(e))

            # å…¶ä»–é”™è¯¯
            logger.exception(f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥ï¼ˆå·¥å…·: {func.__name__}ï¼‰: {e}")
            return ERROR_TOOL_CALL_FAILED.format(error=str(e))

    return async_wrapper


class AbstractLlmService(ABC):
    """
    LLM æœåŠ¡æŠ½è±¡åŸºç±»ã€‚
    
    å®šä¹‰ç»Ÿä¸€çš„ LLM æ¥å£ï¼Œæ”¯æŒä¸åŒçš„æä¾›å•†ï¼ˆOpenAIã€Ollama ç­‰ï¼‰ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡ã€‚"""
        self.llm: Optional[BaseChatModel] = None
        self.agent: Optional[CompiledStateGraph] = None
        self.tools: List[BaseTool] = []
        self._initialize_tools()


    def _initialize_tools(self):
        """åˆå§‹åŒ–å·¥å…·å‡½æ•°åˆ—è¡¨ã€‚"""

        all_functions = [
            # Project ç®¡ç†ï¼ˆåªå…è®¸æŸ¥è¯¢å’Œæ›´æ–°ï¼Œä¸å…è®¸åˆ›å»ºå’Œåˆ é™¤ï¼‰
            get_project, update_project,
            # Actor ç®¡ç†
            create_actor, get_actor, get_all_actors, update_actor,
            remove_actor, add_example, remove_example, add_portrait_from_batch_tool, add_portrait_from_job_tool,
            get_tag_description, get_all_tag_descriptions,
            # Memory ç®¡ç†
            create_memory, get_memory, get_all_memories, update_memory,
            delete_memory, clear_memories, get_key_description, get_all_key_descriptions,
            # Reader / Context åŠŸèƒ½ï¼ˆä½¿ç”¨åˆå¹¶åçš„ context æ¥å£ï¼‰
            get_line, get_chapter_lines, get_lines_range, get_chapters,
            get_chapter, update_chapter, get_stats,
            # é¡¹ç›®/å†…å®¹ç®¡ç†
            get_project_content,
            # å»ºè®®å†™å…¥å·¥å…·ï¼ˆæ”¯æŒè´Ÿç´¢å¼•ï¼‰
            self._add_suggestions,
            # Draw åŠŸèƒ½
            get_loras, get_checkpoints, 
            create_draw_job, create_batch_job, batch_from_jobs,
            get_draw_job, delete_draw_job, get_image,
        ]
        # å…ˆåŒ…è£…ï¼Œå†è½¬æ¢ä¸ºå·¥å…·
        self.tools = [tool(tool_wrapper(func)) for func in all_functions]
        logger.info(f"å·²åˆå§‹åŒ– {len(self.tools)} ä¸ªå·¥å…·å‡½æ•°")

    @abstractmethod
    def initialize_llm(self, response_format: Optional[Any] = None) -> bool:
        """
        åˆå§‹åŒ– LLM å®ä¾‹ã€‚
        
        ç”±å­ç±»å®ç°å…·ä½“çš„ LLM åˆå§‹åŒ–é€»è¾‘ï¼ˆå¦‚ ChatOpenAIã€ChatOllama ç­‰ï¼‰ã€‚
        
        :param response_format: å¯é€‰çš„å“åº”æ ¼å¼ï¼ˆPydantic æ¨¡å‹ç±»ï¼‰ï¼Œç”¨äºç»“æ„åŒ–è¾“å‡º
        :return: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        raise NotImplementedError

    def get_session_context(self, project_id: Optional[str]) -> Optional[str]:
        """
        è·å–å½“å‰é¡¹ç›®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…æ‹¬é¡¹ç›®åŸºæœ¬ä¿¡æ¯å’Œæ‰€æœ‰è®°å¿†æ¡ç›®ï¼‰ã€‚
        
        :param project_id: é¡¹ç›® IDï¼ˆNone è¡¨ç¤ºé»˜è®¤å·¥ä½œç©ºé—´ï¼‰
        :return: æ ¼å¼åŒ–çš„é¡¹ç›®ä¿¡æ¯ï¼ˆJSON å­—ç¬¦ä¸²ï¼‰ï¼Œå¦‚æœé¡¹ç›®ä¸å­˜åœ¨åˆ™è¿”å›é»˜è®¤å·¥ä½œç©ºé—´çš„è®°å¿†ä¿¡æ¯
        """
        try:
            # æŸ¥è¯¢æ‰€æœ‰è®°å¿†æ¡ç›®ï¼ˆæ”¯æŒ project_id ä¸º Noneï¼‰
            memories = MemoryService.get_all(project_id=project_id, limit=1000)

            # æ„å»ºè®°å¿†å­—å…¸ï¼ˆæŒ‰ key åˆ†ç»„ï¼‰
            memories_dict = {}
            for memory in memories:
                memories_dict[memory.key] = {
                    "value": memory.value,
                    "description": memory.description,
                }



            # æŸ¥è¯¢é¡¹ç›®ä¿¡æ¯ï¼ˆé¡¹ç›®ä¸å­˜åœ¨æ—¶è¿”å› Noneï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼‰
            project = ProjectService.get(project_id)
            if not project:
                logger.debug(f"é¡¹ç›®ä¸å­˜åœ¨: {project_id}ï¼Œåªè¿”å›è®°å¿†ä¿¡æ¯")
                # é¡¹ç›®ä¸å­˜åœ¨ï¼Œä½†è¿”å›è®°å¿†ä¿¡æ¯
                default_context = {
                    'project_id': None,
                    "description": "ç›®å‰åœ¨ä¸´æ—¶å·¥ä½œç©ºé—´ä¸­ï¼Œå…·æœ‰ä¸´æ—¶è®°å¿†å’Œä¸´æ—¶è§’è‰²ï¼Œä½†æ˜¯æ²¡æœ‰é¡¹ç›®å†…å®¹"
                                   "æ³¨æ„project_idæ˜¯pythonç±»å‹Noneï¼Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²æˆ–è€…nullä»€ä¹ˆéƒ½"
                }
                session_info = SESSION_INFO_TEMPLATE.format(
                    context_json=json.dumps(default_context, ensure_ascii=False, indent=2),
                    memories_count=len(memories),
                    memories_dict_json=json.dumps(memories_dict, ensure_ascii=False, indent=2),
                )
                return session_info

            # æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = {
                "project_id": project.project_id,
                "title": project.title,
                "novel_path": project.novel_path,
                "total_lines": project.total_lines,
                "total_chapters": project.total_chapters,
                "current_line": project.current_line,
                "current_chapter": project.current_chapter,
            }

            # æ ¼å¼åŒ–ä¸º JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«æç¤ºä¿¡æ¯
            session_info = SESSION_INFO_TEMPLATE.format(
                context_json=json.dumps(context, ensure_ascii=False, indent=2),
                memories_count=len(memories),
                memories_dict_json=json.dumps(memories_dict, ensure_ascii=False, indent=2),
            )

            return session_info

        except Exception as e:
            logger.exception(f"è·å–é¡¹ç›®ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return None

    def build_system_messages(self) -> list[tuple[str, str]]:
        """
        æ„å»ºåŸºç¡€ç³»ç»Ÿæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸åŒ…å«é¡¹ç›®ä¸Šä¸‹æ–‡ï¼‰ã€‚

        :return: åŒ…å«ç³»ç»Ÿæç¤ºè¯çš„å…ƒç»„åˆ—è¡¨
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []

        # 1. å¦‚æœå¯ç”¨å¼€å‘è€…æ¨¡å¼ï¼Œæ·»åŠ å¼€å‘è€…æ¨¡å¼æç¤ºè¯
        if app_settings.llm.developer_mode:
            messages.append(("system", DEVELOP_MODE_PROMPTS))

        # 2. å¦‚æœé…ç½®äº†ç³»ç»Ÿæç¤ºè¯ï¼ˆéç©ºï¼‰ï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if app_settings.llm.system_prompt and app_settings.llm.system_prompt.strip():
            messages.append(("system", app_settings.llm.system_prompt))

        # 3. æ·»åŠ  MCP å·¥å…·ä½¿ç”¨æŒ‡å—ï¼ˆå·²åŒ…å«å·¥å…·è°ƒç”¨æç¤ºå’Œå»ºè®®åŠŸèƒ½è¦æ±‚ï¼‰
        messages.append(("system", MCP_TOOLS_GUIDE))
        
        # 4. æ·»åŠ å»ºè®®æ¡æ•°é…ç½®
        suggestion_config = f"""
**å»ºè®®æ¡æ•°é…ç½®**ï¼š
- **æ–‡å­—å»ºè®®æ¡æ•°**ï¼š{app_settings.llm.text_suggestion_count} æ¡ï¼ˆæ¯æ¬¡å¯¹è¯åæä¾›çš„æ–‡å­—æ“ä½œå»ºè®®æ•°é‡ï¼‰
- **å›¾ç‰‡å»ºè®®æ¡æ•°**ï¼š{app_settings.llm.image_suggestion_count} æ¡ï¼ˆç”Ÿæˆç«‹ç»˜ç­‰å›¾åƒæ—¶æä¾›çš„é€‰é¡¹æ•°é‡ï¼‰
- **âš ï¸ é‡è¦**ï¼šå»ºè®®åªèƒ½æ˜¯çº¯æ–‡å­—æˆ–çº¯å›¾ç‰‡ï¼Œä¸èƒ½åŒæ—¶è¿”å›æ–‡å­—å»ºè®®å’Œå›¾ç‰‡å»ºè®®ã€‚å¦‚æœç”Ÿæˆäº†å›¾ç‰‡ä»»åŠ¡ï¼Œå¿…é¡»åªè¿”å›å›¾ç‰‡å»ºè®®ï¼›å¦åˆ™åªè¿”å›æ–‡å­—å»ºè®®ã€‚
"""
        messages.append(("system", suggestion_config))

        # åˆå¹¶è®°å½•æ‰€æœ‰ç³»ç»Ÿæç¤ºè¯
        if len(messages) > 0:
            logger.debug(
                f"å·²æ·»åŠ  {len(messages)} æ¡åŸºç¡€ç³»ç»Ÿæç¤ºè¯ï¼ˆå¼€å‘è€…æ¨¡å¼ã€ç³»ç»Ÿæç¤ºè¯ã€MCPæŒ‡å—ã€å»ºè®®é…ç½®ï¼‰")
        return messages

    async def chat_invoke(self, message: str, project_id: Optional[str] = None,
                          output_schema: Optional[Any] = None) -> str:
        """
        è°ƒç”¨ LLM å¹¶è¿”å›ç»“æœï¼ˆéæµå¼ï¼Œç”¨äºå·¥å…·è°ƒç”¨ï¼‰ã€‚
        
        :param message: ç”¨æˆ·æ¶ˆæ¯
        :param project_id: é¡¹ç›® IDï¼ˆå¯é€‰ï¼‰
        :param output_schema: è¾“å‡ºæ¨¡å¼ï¼ˆå¯é€‰ï¼ŒPydantic æ¨¡å‹ç±»ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™é»˜è®¤è¿”å›æ–‡æœ¬ï¼‰
        :return: LLM çš„å›å¤å†…å®¹ï¼ˆå¦‚æœæ˜¯ç»“æ„åŒ–è¾“å‡ºï¼Œè¿”å› JSON å­—ç¬¦ä¸²ï¼›å¦åˆ™è¿”å›æ–‡æœ¬ï¼‰
        """
        logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨ LLM: {message[:1000]}{'...' if len(message) > 1000 else ''}")

        try:
            # 1. æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ˆä¸åŒ…å«å†å²æ¶ˆæ¯å’Œæ‘˜è¦ï¼‰
            messages = self.build_system_messages()

            # 2. æ·»åŠ é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ”¯æŒ project_id=Noneï¼Œè¡¨ç¤ºé»˜è®¤å·¥ä½œç©ºé—´ï¼‰
            session_info = self.get_session_context(project_id)
            if session_info:
                messages.append(("system", session_info))

            # 3. å¦‚æœ project_id ä¸º Noneï¼Œæ·»åŠ å·¥å…·ä½¿ç”¨è¯´æ˜
            if project_id is None:
                from api.constants.llm import NO_PROJECT_ID_WARNING
                messages.append(("system", NO_PROJECT_ID_WARNING))

            # 4. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append(("human", message))

            # 5. å¦‚æœæœ‰è¾“å‡º schemaï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ– agent ä»¥æ”¯æŒç»“æ„åŒ–è¾“å‡º
            if output_schema is not None:
                logger.info(
                    f"ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼Œschema: {output_schema.__name__ if hasattr(output_schema, '__name__') else type(output_schema)}")
                # é‡æ–°åˆå§‹åŒ– agent ä»¥æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼ˆåŒæ—¶ä¿ç•™å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼‰
            self.initialize_llm(response_format=output_schema)

            # 6. è°ƒç”¨ agentï¼ˆä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼‰
            logger.info(f"å¼€å§‹è°ƒç”¨ LLMï¼Œä½¿ç”¨ {len(self.tools)} ä¸ªå·¥å…·" + (
                f"ï¼Œç»“æ„åŒ–è¾“å‡º: {output_schema.__name__ if output_schema else 'æ— '}" if output_schema else ""))
            config = {"recursion_limit": app_settings.llm.recursion_limit}
            result: dict = await self.agent.ainvoke({"messages": messages}, config=config, stream_mode='values')
            result_message: AIMessage = result['messages'][-1]
            context = result_message.content  # json text

            # 7. å¦‚æœæœ‰ç»“æ„åŒ–è¾“å‡ºï¼Œç›´æ¥è¿”å›æ–‡æœ¬ï¼ˆLLM å·²ç»è¿”å›äº† JSONï¼‰
            if output_schema is not None:
                logger.info(f"âœ… LLM è°ƒç”¨å®Œæˆï¼Œè¿”å›ç»“æ„åŒ–è¾“å‡ºï¼Œé•¿åº¦={len(context)}")
                return context

            # 8. å¦‚æœæ²¡æœ‰ç»“æ„åŒ–è¾“å‡ºï¼Œè¿”å›æ™®é€šæ–‡æœ¬å†…å®¹
            logger.info(f"âœ… LLM è°ƒç”¨å®Œæˆï¼Œè¿”å›é•¿åº¦={len(context)}")
            logger.debug(f"æœ€ç»ˆæå–çš„å†…å®¹: {context[:500] if context else '(ç©º)'}")
            return context

        except Exception as e:
            logger.exception(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            return f"é”™è¯¯ï¼šLLM è°ƒç”¨å¤±è´¥ - {str(e)}"

    def summary_history(self, project_id: Optional[str]):
        """
        è‡ªåŠ¨ç”ŸæˆèŠå¤©æ‘˜è¦ï¼ˆå½“æ¶ˆæ¯æ•°é‡è¾¾åˆ° summary_epoch çš„å€æ•°æ—¶ï¼‰ã€‚
        
        :param project_id: é¡¹ç›®IDï¼ˆNone è¡¨ç¤ºé»˜è®¤å·¥ä½œç©ºé—´ï¼Œä¸ç”Ÿæˆæ‘˜è¦ï¼‰
        """
        # æ— é¡¹ç›®æ—¶ä¸ç”Ÿæˆæ‘˜è¦ï¼ˆChatSummary çš„ project_id æ˜¯ä¸»é”®ï¼Œä¸èƒ½ä¸º Noneï¼‰
        if project_id is None:
            return
        count = HistoryService.count(project_id)
        res_round = count % app_settings.llm.summary_epoch

        if count > 0 and res_round == 0:
            try:
                # è·å–æœ€è¿‘çš„æ¶ˆæ¯
                start_index = count - app_settings.llm.summary_epoch
                messages = self.build_system_messages()

                # æ·»åŠ é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
                session_info = self.get_session_context(project_id)
                if session_info:
                    messages.append(("system", session_info))

                recent_messages = HistoryService.get_all(project_id, start_index=start_index, end_index=count)

                # è·å–ç°æœ‰æ‘˜è¦
                chat_summary = MemoryService.get_summary(project_id)
                summary_value = chat_summary.data if chat_summary and chat_summary.data else ""

                # æ„å»ºæ‘˜è¦æç¤ºè¯
                summary_message = SUMMARY_MESSAGE_TEMPLATE.format(
                    previous_rounds=count - app_settings.llm.summary_epoch,
                    summary_value=summary_value,
                    recent_rounds=app_settings.llm.summary_epoch,
                    recent_messages=json.dumps(
                        [{"role": msg.role, "context": msg.context} for msg in recent_messages],
                        ensure_ascii=False,
                        indent=2
                    ),
                )
                messages.append(("system", summary_message))

                # è°ƒç”¨ agent ç”Ÿæˆæ‘˜è¦
                result = self.agent.invoke({"messages": messages})

                # æå–æ‘˜è¦æ–‡æœ¬
                summary_text = ""
                if hasattr(result, "messages") and result.messages:
                    last_msg = result.messages[-1]
                    if hasattr(last_msg, "content"):
                        summary_text = last_msg.content
                    elif isinstance(last_msg, dict) and "content" in last_msg:
                        summary_text = last_msg["content"]

                # ä¿å­˜æ‘˜è¦
                if summary_text:
                    MemoryService.update_summary(project_id, summary_text)
                    logger.info(f"å·²ç”ŸæˆèŠå¤©æ‘˜è¦: {project_id}, é•¿åº¦={len(summary_text)}")
            except Exception as e:
                logger.exception(f"ç”ŸæˆèŠå¤©æ‘˜è¦å¤±è´¥: {e}")

    async def chat_streamed(self, message: str, project_id: Optional[str] = None) -> AsyncGenerator[dict, None]:
        """
        å¢å¼ºçš„æµå¼å¯¹è¯æ–¹æ³•ï¼Œè¿”å›ç»“æ„åŒ–äº‹ä»¶ã€‚
        
        :param message: ç”¨æˆ·æ¶ˆæ¯
        :param project_id: é¡¹ç›® ID
        :yield: äº‹ä»¶å­—å…¸ï¼ŒåŒ…å« type å’Œç›¸åº”çš„æ•°æ®
        """
        self.initialize_llm()
        logger.info(f"ğŸ‘¤ ç”¨æˆ·æ¶ˆæ¯: {message[:200]}{'...' if len(message) > 200 else ''}")

        # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯çš„ IDï¼ˆæå‰åˆ›å»ºï¼Œç”¨äºå®æ—¶æ›´æ–°ï¼‰
        assistant_context = ""
        assistant_tools: list[dict] = []
        assistant_suggests: list[str] = []

        try:
            # 1. æ„å»ºç³»ç»Ÿæ¶ˆæ¯å’Œå†å²æ¶ˆæ¯
            messages = self.build_system_messages()

            # æ·»åŠ é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
            session_info = self.get_session_context(project_id)
            if session_info:
                messages.append(("system", session_info))

            # ç”Ÿæˆæ‘˜è¦ï¼ˆä»…åœ¨æœ‰é¡¹ç›®æ—¶ï¼‰
            if project_id:
                self.summary_history(project_id)
                chat_summary_memory = MemoryService.get_summary(project_id)
            else:
                chat_summary_memory = None

            # 2. åˆ›å»ºç”¨æˆ·æ¶ˆæ¯å¹¶å†™å…¥æ•°æ®åº“ï¼ˆID ä¼šè‡ªåŠ¨ç”Ÿæˆï¼‰
            user_message = ChatMessage(
                project_id=project_id,
                role="user",
                context=message,
                status="ready",
                message_type="normal"
            )
            user_message = HistoryService.create(user_message)

            # 3. è·å–å†å²æ¶ˆæ¯ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
            start_index = max(0, HistoryService.count(project_id) - app_settings.llm.summary_epoch)
            messages_to_include = HistoryService.get_all(project_id, start_index=start_index)

            # 4. å¦‚æœæœ‰èŠå¤©æ‘˜è¦ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯
            if chat_summary_memory and chat_summary_memory.data:
                summary_message = SUMMARY_MESSAGE_TEMPLATE.format(
                    previous_rounds=start_index,
                    summary_value=chat_summary_memory.data,
                    recent_rounds=len(messages_to_include),
                )
                messages.append(("system", summary_message))

            # 5. æ·»åŠ å†å²æ¶ˆæ¯åˆ° langchain æ¶ˆæ¯åˆ—è¡¨
            for msg in messages_to_include:
                if msg.role == "user":
                    messages.append(("human", msg.context))
                elif msg.role == "assistant":
                    messages.append(("ai", msg.context))

            # 6. åˆ›å»ºåˆå§‹åŠ©æ‰‹æ¶ˆæ¯ï¼ˆçŠ¶æ€ä¸º thinkingï¼ŒID ä¼šè‡ªåŠ¨ç”Ÿæˆï¼‰
            assistant_message = ChatMessage(
                project_id=project_id,
                role="assistant",
                context="",
                status="thinking",
                message_type="normal",
                tools=[],
                suggests=[]
            )
            assistant_message = HistoryService.create(assistant_message)
            assistant_message_id = assistant_message.message_id

            # å‘é€æ¶ˆæ¯ID
            yield {'type': 'message_id', 'message_id': assistant_message_id}
            yield {'type': 'status', 'status': 'thinking'}

            # 7. å®ç°æµå¼å¯¹è¯é€»è¾‘
            logger.info(f"å¼€å§‹æµå¼å¯¹è¯ï¼Œä½¿ç”¨ {len(self.tools)} ä¸ªå·¥å…·")
            config = {"recursion_limit": app_settings.llm.recursion_limit}

            async for chunk in self.agent.astream_events(
                    {"messages": messages},
                    version="v2",
                    config=config
            ):
                event_type = chunk.get("event")

                # å¤„ç†æ–‡æœ¬æµäº‹ä»¶
                if event_type == "on_chat_model_stream":
                    message_chunk = chunk.get("data", {}).get("chunk")
                    if message_chunk and hasattr(message_chunk, "content") and message_chunk.content:
                        content = message_chunk.content
                        assistant_context += content

                        # å®æ—¶æ›´æ–°æ•°æ®åº“ä¸­çš„åŠ©æ‰‹æ¶ˆæ¯
                        assistant_message.context = assistant_context
                        assistant_message.tools = assistant_tools.copy()
                        HistoryService.update(assistant_message)

                        yield {'type': 'content', 'content': content}

                # å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶
                elif event_type == "on_tool_start":
                    tool_name = chunk.get("name", "")
                    
                    # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå†…éƒ¨å‡½æ•°ä¸ä¼šè¢«æ·»åŠ åˆ°åˆ—è¡¨ï¼‰
                    self._process_tool_start_event(chunk, assistant_tools)

                    # æ›´æ–°æ•°æ®åº“
                    assistant_message.tools = assistant_tools.copy()
                    HistoryService.update(assistant_message)

                    # åªä¸ºéå†…éƒ¨å‡½æ•°å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶
                    if not tool_name.startswith("_"):
                        tool_input = chunk.get("data", {}).get("input", {})
                        yield {
                            'type': 'tool_start',
                            'name': tool_name,
                            'args': tool_input if isinstance(tool_input, dict) else {}
                        }
                        # å‘é€å®Œæ•´çš„å·¥å…·åˆ—è¡¨
                        yield {'type': 'tools', 'tools': assistant_tools.copy()}

                # å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸäº‹ä»¶
                elif event_type == "on_tool_end":
                    tool_name = chunk.get("name", "")
                    tool_output: ToolMessage = chunk.get("data", {}).get("output")
                    logger.info(
                        f"âœ… å·¥å…·è°ƒç”¨å®Œæˆ: {tool_name}, ç»“æœé•¿åº¦={len(str(tool_output.content)) if tool_output.content else 0}")

                    # å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸï¼ˆå†…éƒ¨å‡½æ•°ä¸ä¼šè¢«æ›´æ–°åˆ°åˆ—è¡¨ï¼‰
                    self._process_tool_end_event(chunk, assistant_tools)

                    # æ›´æ–°æ•°æ®åº“
                    assistant_message.tools = assistant_tools.copy()
                    HistoryService.update(assistant_message)

                    # åªä¸ºéå†…éƒ¨å‡½æ•°å‘é€å·¥å…·è°ƒç”¨ç»“æŸäº‹ä»¶
                    if not tool_name.startswith("_"):
                        yield {
                            'type': 'tool_end',
                            'name': tool_name,
                            'result': tool_output.content
                        }
                        # å‘é€å®Œæ•´çš„å·¥å…·åˆ—è¡¨
                        yield {'type': 'tools', 'tools': assistant_tools.copy()}

                    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœå·¥å…·æ˜¯ add_choicesï¼Œæ›´æ–° suggests
                    if tool_name == "add_choices":
                        from api.routers.llm import _session_choices
                        choices = _session_choices.get(project_id, [])
                        suggests = []
                        for choice in choices:
                            if isinstance(choice, dict):
                                if choice.get("type") == "image":
                                    suggests.append(f"image:{choice.get('url', '')}")
                                elif choice.get("type") == "text":
                                    suggests.append(choice.get("text", ""))
                            elif isinstance(choice, str):
                                suggests.append(choice)
                        assistant_message.suggests = suggests
                        assistant_suggests = suggests
                        HistoryService.update(assistant_message)

                        # å‘é€å»ºè®®æ›´æ–°
                        yield {'type': 'suggests', 'suggests': suggests}

            # 8. å¯¹è¯å®Œæˆï¼Œæ›´æ–°åŠ©æ‰‹æ¶ˆæ¯çŠ¶æ€ä¸º ready
            assistant_message.status = "ready"
            assistant_message.context = assistant_context
            assistant_message.tools = assistant_tools.copy()
            HistoryService.update(assistant_message)

            yield {'type': 'status', 'status': 'ready'}
            logger.info(f"âœ… å¯¹è¯å®Œæˆ: {len(assistant_context)} å­—ç¬¦, {len(assistant_tools)} ä¸ªå·¥å…·è°ƒç”¨")

        except Exception as e:
            logger.exception(f"å¯¹è¯å¤±è´¥: {e}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯æˆ–è¶…æ—¶é”™è¯¯
            error_type = type(e).__name__
            error_str = str(e).lower()

            is_connection_error = (
                    "connection" in error_str or
                    "connect" in error_str or
                    "è¿æ¥" in error_str or
                    "network" in error_str or
                    "ç½‘ç»œ" in error_str or
                    error_type in ("APIConnectionError", "ConnectError", "ConnectTimeout", "ReadTimeout")
            )

            is_timeout_error = (
                    "timeout" in error_str or
                    "è¶…æ—¶" in error_str or
                    error_type in ("TimeoutError", "ConnectTimeout", "ReadTimeout")
            )

            if is_connection_error or is_timeout_error:
                if is_timeout_error:
                    error_msg = ERROR_TIMEOUT_TEMPLATE.format(timeout=app_settings.llm.timeout)
                else:
                    error_msg = ERROR_CONNECTION_TEMPLATE
            else:
                error_msg = f"é”™è¯¯ï¼š{e}"

            # æ›´æ–°åŠ©æ‰‹æ¶ˆæ¯ä¸ºé”™è¯¯çŠ¶æ€ï¼ˆå¦‚æœå·²åˆ›å»ºï¼‰
            if 'assistant_message' in locals() and assistant_message:
                try:
                    assistant_message.status = "error"
                    assistant_message.context = error_msg
                    HistoryService.update(assistant_message)
                except Exception as update_error:
                    logger.warning(f"æ›´æ–°åŠ©æ‰‹æ¶ˆæ¯å¤±è´¥: {update_error}")

            yield {'type': 'status', 'status': 'error'}
            yield {'type': 'error', 'error': error_msg}

    async def chat_text_only(self, message: str, project_id: str) -> AsyncGenerator[str, None]:
        """
        ä¸ LLM è¿›è¡Œæ ‡å‡†å¯¹è¯ï¼ˆæµå¼è¿”å›ï¼‰ã€‚
        
        è¿™æ˜¯ `chat_streamed` çš„åŒ…è£…å™¨ï¼Œåªè¿”å›æ–‡æœ¬å†…å®¹ç‰‡æ®µã€‚
        å†…éƒ¨è°ƒç”¨ `chat_streamed` å¹¶æå– `content` äº‹ä»¶ã€‚
        
        :param message: ç”¨æˆ·æ¶ˆæ¯
        :param project_id: é¡¹ç›® ID
        :yield: LLM å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        self.initialize_llm()
        async for event in self.chat_streamed(message, project_id):
            if event.get('type') == 'content':
                yield event.get('content', '')
            elif event.get('type') == 'error':
                yield event.get('error', 'é”™è¯¯ï¼šæœªçŸ¥é”™è¯¯')
                return

    async def chat_iteration(self, iteration_data: dict, project_id: str) -> AsyncGenerator[str, None]:
        """
        è¿­ä»£æ¨¡å¼ä¸“ç”¨æ–¹æ³•ã€‚
        
        å®Œå…¨åŸºäºæ•°æ®åº“æ“ä½œï¼š
        - è¿­ä»£æ•°æ®å­˜å‚¨åœ¨ ChatMessage çš„ data å­—æ®µä¸­ï¼ˆChatIteration å¯¹è±¡ï¼‰
        - å®æ—¶æ›´æ–°è¿­ä»£è¿›åº¦åˆ°æ•°æ®åº“
        - æœ€ç»ˆæ“ä½œçš„å·¥å…·è°ƒç”¨è®°å½•åˆ° tools å­—æ®µ
        
        é€šç”¨è¿­ä»£ç®¡ç†ï¼š
        1. æ¯æ¬¡è¿­ä»£ï¼Œè®© LLM è‡ªè¡Œè°ƒç”¨å·¥å…·å¤„ç†å½“å‰ index çš„å†…å®¹
        2. LLM å¤„ç†å®Œæˆåï¼Œæ›´æ–° index += step
        3. ç´¯ç§¯ summary
        4. å½“ index >= stop æ—¶ï¼Œæ‰§è¡Œæœ€ç»ˆæ“ä½œå¹¶é€€å‡º
        
        :param iteration_data: è¿­ä»£æ•°æ®å­—å…¸ï¼ˆåŒ…å« message_id æˆ–å®Œæ•´çš„è¿­ä»£ä¿¡æ¯ï¼‰
        :param project_id: é¡¹ç›®ID
        :yield: LLMå“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        self.initialize_llm()
        # 1. è·å–æˆ–åˆ›å»ºè¿­ä»£æ¶ˆæ¯
        if "message_id" in iteration_data:
            # ä»æ•°æ®åº“è¯»å–ç°æœ‰è¿­ä»£æ¶ˆæ¯
            iteration_message = HistoryService.get(iteration_data["message_id"])
            if not iteration_message or iteration_message.project_id != project_id:
                logger.error(f"è¿­ä»£æ¶ˆæ¯ä¸å­˜åœ¨: {iteration_data['message_id']}")
                yield "é”™è¯¯ï¼šè¿­ä»£æ¶ˆæ¯ä¸å­˜åœ¨"
                return
            iteration = ChatIteration(**iteration_message.data)
        else:
            # åˆ›å»ºæ–°çš„è¿­ä»£æ¶ˆæ¯ï¼ˆID ä¼šè‡ªåŠ¨ç”Ÿæˆï¼‰
            iteration = ChatIteration(**iteration_data)
            iteration_message = ChatMessage(
                project_id=project_id,
                role="assistant",
                context="",
                status="thinking",
                message_type="iteration",
                data=iteration.model_dump(),
                tools=[],
                suggests=[]
            )
            iteration_message = HistoryService.create(iteration_message)

        # 2. è¿­ä»£å¾ªç¯
        while iteration.index < iteration.stop:
            # æ„å»ºè¿­ä»£æ¨¡å¼ä¸“ç”¨æç¤ºè¯
            iteration_prompt = self._build_iteration_prompt(iteration, project_id)

            # è°ƒç”¨LLMå¤„ç†å½“å‰è¿­ä»£ï¼ˆè¿­ä»£è¿‡ç¨‹ä¸­ä¸è®°å½•å·¥å…·è°ƒç”¨ï¼Œåªç´¯ç§¯summaryï¼‰
            full_response = ""
            async for chunk in self._call_llm_in_iteration_mode(iteration_prompt, project_id):
                full_response += chunk
                yield chunk

            # æ›´æ–°summaryï¼ˆå°†LLMçš„å“åº”è¿½åŠ åˆ°summaryï¼‰
            if iteration.summary:
                iteration.summary += "\n\n"
            iteration.summary += f"[ç¬¬ {iteration.index // iteration.step + 1} æ¬¡è¿­ä»£] {full_response}"

            # æ›´æ–°indexï¼ˆå°†æ­¥é•¿å åŠ åˆ°indexï¼‰
            iteration.index += iteration.step

            # æ›´æ–°æ•°æ®åº“ä¸­çš„è¿­ä»£æ¶ˆæ¯
            iteration_message.data = iteration.model_dump()
            iteration_message.status = "thinking"
            HistoryService.update(iteration_message)

            logger.debug(
                f"å·²æ›´æ–°è¿­ä»£æ¶ˆæ¯è¿›åº¦: index={iteration.index}/{iteration.stop}, "
                f"summaryé•¿åº¦={len(iteration.summary) if iteration.summary else 0}")

            # æ£€æŸ¥æ˜¯å¦è¿­ä»£ç»ˆæ­¢
            if iteration.index >= iteration.stop:
                break

        # 3. è¿­ä»£å®Œæˆï¼Œæ‰§è¡Œæœ€ç»ˆæ“ä½œ
        logger.info(f"è¿­ä»£å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œæœ€ç»ˆæ“ä½œï¼š{iteration.target}")
        final_prompt = self._build_final_operation_prompt(iteration)

        # æ¸…ç©ºä¹‹å‰çš„å·¥å…·è°ƒç”¨ï¼ˆåªä¿ç•™æœ€ç»ˆæ“ä½œçš„å·¥å…·è°ƒç”¨ï¼‰
        final_tools: list[dict] = []
        final_context_ref = [""]  # ä½¿ç”¨åˆ—è¡¨å¼•ç”¨ä»¥ä¾¿ä¿®æ”¹

        # 4. è°ƒç”¨LLMæ‰§è¡Œæœ€ç»ˆæ“ä½œï¼ˆè®°å½•å·¥å…·è°ƒç”¨ï¼‰
        async for chunk in self._call_llm_final_operation(final_prompt, project_id, final_tools, final_context_ref):
            yield chunk

        # 5. æ›´æ–°è¿­ä»£æ¶ˆæ¯ï¼ˆæ ‡è®°ä¸ºå®Œæˆï¼ŒåŒ…å«æœ€ç»ˆæ“ä½œçš„å·¥å…·è°ƒç”¨ï¼‰
        iteration_message.status = "ready"
        iteration_message.context = final_context_ref[0]
        iteration_message.tools = final_tools.copy()
        iteration_message.data = iteration.model_dump()
        HistoryService.update(iteration_message)

        logger.info(
            f"âœ… è¿­ä»£å®Œæˆ: index={iteration.index}/{iteration.stop}, "
            f"summaryé•¿åº¦={len(iteration.summary)}, å·¥å…·è°ƒç”¨æ•°={len(final_tools)}")

    def _build_iteration_prompt(self, iteration: ChatIteration, project_id: str) -> str:
        """æ„å»ºè¿­ä»£æ¨¡å¼æç¤ºè¯"""
        progress_percent = iteration.index * 100 // iteration.stop if iteration.stop > 0 else 0
        is_near_completion = "æ˜¯" if iteration.index + iteration.step >= iteration.stop else "å¦"
        summary_display = iteration.summary if iteration.summary else "ï¼ˆæš‚æ— ï¼‰"

        return ITERATION_PROMPT_TEMPLATE.format(
            target=iteration.target,
            index=iteration.index,
            step=iteration.step,
            stop=iteration.stop,
            progress_percent=progress_percent,
            is_near_completion=is_near_completion,
            summary_display=summary_display,
        )

    def _build_final_operation_prompt(self, iteration: ChatIteration) -> str:
        """æ„å»ºæœ€ç»ˆæ“ä½œæç¤ºè¯"""
        iterations_count = iteration.stop // iteration.step if iteration.step > 0 else 0

        return FINAL_OPERATION_PROMPT_TEMPLATE.format(
            target=iteration.target,
            stop=iteration.stop,
            iterations_count=iterations_count,
            summary=iteration.summary,
        )

    async def _call_llm_in_iteration_mode(self, prompt: str, project_id: str) -> AsyncGenerator[str, None]:
        """
        åœ¨è¿­ä»£æ¨¡å¼ä¸‹è°ƒç”¨LLMï¼ˆä¸è®°å½•åˆ°historyï¼Œåªç”¨äºç´¯ç§¯summaryï¼‰ã€‚
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if app_settings.llm.developer_mode:
            messages.append(("system", DEVELOP_MODE_PROMPTS))

        if app_settings.llm.system_prompt:
            messages.append(("system", app_settings.llm.system_prompt))

        # æ·»åŠ è¿­ä»£æ¨¡å¼ä¸“ç”¨æŒ‡å—
        messages.append(("system", ITERATION_GUIDE))

        # æ·»åŠ å½“å‰æç¤ºè¯
        messages.append(("human", prompt))

        # é…ç½®é€’å½’é™åˆ¶ï¼ˆç”¨äºè¿­ä»£æ¨¡å¼ç­‰éœ€è¦å¤§é‡å·¥å…·è°ƒç”¨çš„åœºæ™¯ï¼‰
        config = {"recursion_limit": app_settings.llm.recursion_limit}

        # è°ƒç”¨LLMï¼ˆä¸è®°å½•åˆ°historyï¼‰
        full_response = ""
        async for chunk in self.agent.astream_events({"messages": messages}, version="v2", config=config):
            event_type = chunk.get("event")

            if event_type == "on_chat_model_stream":
                message_chunk = chunk.get("data", {}).get("chunk")
                if message_chunk and hasattr(message_chunk, "content"):
                    content = message_chunk.content
                    if content:
                        full_response += content
                        yield content

        # è¿­ä»£è¿‡ç¨‹ä¸­çš„å·¥å…·è°ƒç”¨ä¸éœ€è¦è®°å½•ï¼ˆç”¨æˆ·è¦æ±‚ï¼‰

    def _process_tool_start_event(self, chunk: dict, tools_list: list, log_prefix: str = "") -> None:
        """
        å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶çš„é€šç”¨æ–¹æ³•ã€‚
        
        :param chunk: äº‹ä»¶ chunk
        :param tools_list: å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆä¼šè¢«æ›´æ–°ï¼‰
        :param log_prefix: æ—¥å¿—å‰ç¼€ï¼ˆå¯é€‰ï¼‰
        """
        tool_name = chunk.get("name", "")
        tool_input = chunk.get("data", {}).get("input", {})
        prefix = f"[{log_prefix}] " if log_prefix else ""
        logger.info(f"âœ… {prefix}å·¥å…·è°ƒç”¨: {tool_name}, å‚æ•°: {tool_input}")

        # è·³è¿‡ä»¥ _ å¼€å¤´çš„å†…éƒ¨å‡½æ•°ï¼ˆå¦‚ _add_suggestionsï¼‰
        if tool_name.startswith("_"):
            logger.debug(f"è·³è¿‡å†…éƒ¨å‡½æ•°å·¥å…·è°ƒç”¨è®°å½•: {tool_name}")
            return

        args = tool_input if isinstance(tool_input, dict) else {}
        if 'request' in args and len(args) == 1:
            args = args['request']

        tool_call = {
            "name": tool_name,
            "args": args,
            "result": None
        }
        tools_list.append(tool_call)

    def _process_tool_end_event(self, chunk: dict, tools_list: list, log_prefix: str = "") -> None:
        """
        å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸäº‹ä»¶çš„é€šç”¨æ–¹æ³•ã€‚
        
        :param chunk: äº‹ä»¶ chunk
        :param tools_list: å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆä¼šè¢«æ›´æ–°ï¼‰
        :param log_prefix: æ—¥å¿—å‰ç¼€ï¼ˆå¯é€‰ï¼‰
        """
        tool_name = chunk.get("name", "")
        tool_output: ToolMessage = chunk.get("data", {}).get("output")
        prefix = f"[{log_prefix}] " if log_prefix else ""
        logger.info(f"âœ… {prefix}å·¥å…·è°ƒç”¨å®Œæˆ: {tool_name}")

        # è·³è¿‡ä»¥ _ å¼€å¤´çš„å†…éƒ¨å‡½æ•°ï¼ˆå¦‚ _add_suggestionsï¼‰
        if tool_name.startswith("_"):
            logger.debug(f"è·³è¿‡å†…éƒ¨å‡½æ•°å·¥å…·è°ƒç”¨ç»“æœè®°å½•: {tool_name}")
            return

        if tools_list:
            try:
                result = json.loads(tool_output.content)
            except (json.JSONDecodeError, TypeError):
                # å¯èƒ½æ˜¯åŸºæœ¬ç±»å‹ï¼Œæ¯”å¦‚å­—ç¬¦ä¸²
                result = tool_output.content
            tools_list[-1]["result"] = result
            tools_list[-1]["status"] = tool_output.status
            tools_list[-1]["tool_call_id"] = tool_output.tool_call_id

    async def _add_suggestions(self, project_id: Optional[str], index: int, suggests: list) -> None:
        """
        å°†å»ºè®®å†™å…¥æŒ‡å®šç´¢å¼•çš„æ¶ˆæ¯ï¼ˆæ”¯æŒè´Ÿç´¢å¼•ï¼‰ã€‚

        è¯´æ˜ï¼šéƒ¨åˆ†ä»£ç ä¼šä»¥è´Ÿç´¢å¼•è¡¨ç¤ºä»æœ«å°¾å€’æ•°çš„ä½ç½®ï¼ˆä¾‹å¦‚ -1 è¡¨ç¤ºæœ€åä¸€æ¡æ¶ˆæ¯ï¼‰ï¼Œ
        è¿™é‡Œéœ€è¦å°†è´Ÿç´¢å¼•è½¬æ¢ä¸ºå®é™…ç´¢å¼•åå†è¿›è¡Œæ›´æ–°ã€‚

        :param project_id: ä¼šè¯/é¡¹ç›® IDï¼ˆNone è¡¨ç¤ºé»˜è®¤å·¥ä½œç©ºé—´ï¼‰
        :param index: æ¶ˆæ¯ç´¢å¼•ï¼Œæ”¯æŒè´Ÿæ•°ï¼ˆ-1 è¡¨ç¤ºæœ€åä¸€æ¡ï¼‰
        :param suggests: å»ºè®®åˆ—è¡¨
        """
        if project_id=='null':
            project_id=None
        try:
            # è´Ÿç´¢å¼•æ”¯æŒï¼šå°† -1 è¡¨ç¤ºæœ€åä¸€æ¡è½¬æ¢ä¸ºå®é™…ç´¢å¼•
            if index < 0:
                count = HistoryService.count(project_id)
                index = count + index

            # è·å–ç›®æ ‡æ¶ˆæ¯å¹¶æ›´æ–° suggests
            message = HistoryService.get_by_index(project_id, index)
            if not message:
                logger.debug(f"å°è¯•ä¸ºä¸å­˜åœ¨çš„æ¶ˆæ¯æ·»åŠ å»ºè®®: project={project_id}, index={index}")
                return

            message.suggests = suggests or []
            HistoryService.update(message)
            logger.debug(f"å·²ä¸ºæ¶ˆæ¯æ·»åŠ å»ºè®®: project={project_id}, index={index}, suggests_count={len(message.suggests)}")
        except Exception as e:
            logger.exception(f"æ·»åŠ å»ºè®®å¤±è´¥: project={project_id}, index={index}, error={e}")

    async def _call_llm_final_operation(
            self,
            prompt: str,
            project_id: str,
            tools_list: list,
            context_ref: list
    ) -> AsyncGenerator[str, None]:
        """
        è°ƒç”¨LLMæ‰§è¡Œæœ€ç»ˆæ“ä½œï¼ˆå…è®¸æ‰€æœ‰å·¥å…·ï¼Œè®°å½•åˆ°ä¼ å…¥çš„åˆ—è¡¨ï¼‰ã€‚
        
        è¿™æ˜¯ `chat_streamed` çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ¶ˆæ¯åˆ—è¡¨ï¼Œä¸å†™å…¥æ•°æ®åº“ã€‚
        
        :param prompt: æç¤ºè¯
        :param project_id: é¡¹ç›®ID
        :param tools_list: å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆä¼šè¢«å®æ—¶æ›´æ–°ï¼‰
        :param context_ref: ä¸Šä¸‹æ–‡å†…å®¹çš„å¼•ç”¨ï¼ˆåˆ—è¡¨ï¼Œç”¨äºä¿®æ”¹å­—ç¬¦ä¸²ï¼‰
        """
        # æ„å»ºè‡ªå®šä¹‰æ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸åŒ…å«å†å²æ¶ˆæ¯å’ŒèŠå¤©æ‘˜è¦ï¼‰
        messages = []

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if app_settings.llm.developer_mode:
            messages.append(("system", DEVELOP_MODE_PROMPTS))

        if app_settings.llm.system_prompt:
            messages.append(("system", app_settings.llm.system_prompt))

        # æ·»åŠ  MCP å·¥å…·ä½¿ç”¨æŒ‡å—
        messages.append(("system", MCP_TOOLS_GUIDE))

        # æ·»åŠ å½“å‰é¡¹ç›®ä¿¡æ¯
        session_info = self.get_session_context(project_id)
        if session_info:
            messages.append(("system", session_info))

        # æ·»åŠ å½“å‰æç¤ºè¯
        messages.append(("human", prompt))

        # é…ç½®é€’å½’é™åˆ¶
        config = {"recursion_limit": app_settings.llm.recursion_limit}

        # è°ƒç”¨LLMå¹¶å¤„ç†äº‹ä»¶
        async for chunk in self.agent.astream_events({"messages": messages}, version="v2", config=config):
            event_type = chunk.get("event")

            if event_type == "on_chat_model_stream":
                message_chunk = chunk.get("data", {}).get("chunk")
                if message_chunk and hasattr(message_chunk, "content") and message_chunk.content:
                    content = message_chunk.content
                    context_ref[0] += content
                    yield content

            elif event_type == "on_tool_start":
                self._process_tool_start_event(chunk, tools_list, "æœ€ç»ˆæ“ä½œ")

            elif event_type == "on_tool_end":
                self._process_tool_end_event(chunk, tools_list, "æœ€ç»ˆæ“ä½œ")
